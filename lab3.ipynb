{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "donate.wikimedia.org\n",
      "www.wikidata.org\n",
      "www.wikidata.org\n",
      "commons.wikimedia.org\n",
      "www.nationalparks.nsw.gov.au\" rel=\"nofollow\">www<wbr\n",
      "www.nationalparks.nsw.gov.au\n",
      "www.legislation.nsw.gov.au\n",
      "web.archive.org\n",
      "au.news.yahoo.com\n",
      "au.news.yahoo.com\n",
      "fnpw.org.au\n",
      "scholar.lib.vt.edu\n",
      "www.austlii.edu.au\n",
      "www.nationalparks.nsw.gov.au\n",
      "records-primo.hosted.exlibrisgroup.com\n",
      "records-primo.hosted.exlibrisgroup.com\n",
      "records-primo.hosted.exlibrisgroup.com\n",
      "www.environment.nsw.gov.au\n",
      "www.nationalparks.nsw.gov.au\n",
      "www.nationalparks.nsw.gov.au\n",
      "www.nationalparks.nsw.gov.au\n",
      "www.environment.nsw.gov.au\n",
      "www.environment.nsw.gov.au\n",
      "www.nationalparks.nsw.gov.au\n",
      "www.wikidata.org\n",
      "isni.org\n",
      "viaf.org\n",
      "id.loc.gov\n",
      "aleph.nkp.cz\n",
      "en.wikipedia.org\n",
      "foundation.wikimedia.org\n",
      "foundation.wikimedia.org\n",
      "developer.wikimedia.org\">Developers<\n",
      "stats.wikimedia.org\n",
      "foundation.wikimedia.org\n",
      "wikimediafoundation.org\n",
      "www.mediawiki.org\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/NSW_National_Parks_%26_Wildlife_Service\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\") #turns the HTML content of the website into a variable\n",
    "links = soup.find_all(\"a\") #finds everything in the HTML that has an a tag\n",
    "all = re.findall(r\"://[^.]*[^.]*[^/]*\",str(links)) #looks through all of those tags for links and puts them into a list\n",
    "for i in all: #takes out all links from the list that go to other wikipedia pages\n",
    "    if \"wikipedia\" in i:\n",
    "        all.remove(i)\n",
    "for i in all: #removes the \"://\" from each link\n",
    "    print(i.strip(\"://\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu 15 High:40 Low:26\n",
      "Fri 16 High:48 Low:23\n",
      "Sat 17 High:53 Low:26\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "page = requests.get(\"https://weather.com/weather/tenday/l/Centennial+CO?canonicalCityId=e769bdf85f5875e5849c520db728bce588d28b45cf3fac2debcba798c3101422\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")#takes the html code from weather.com\n",
    "days = soup.find_all(\"h3\", class_=\"DetailsSummary--daypartName--kbngc\") #finding the days using the classname\n",
    "days = re.findall(r\"[a-zA-Z]{3} \\d{2}\", str(days)) #isolates the day and number\n",
    "high = soup.find_all(\"span\", class_=\"DetailsSummary--highTempValue--3PjlX\") #finding the high temperatures using the classname\n",
    "high = re.findall(r\"\\d{2}\", str(high)) #isolates the value\n",
    "low = soup.find_all(\"span\", class_=\"DetailsSummary--lowTempValue--2tesQ\") #finding the low temperature using the classname\n",
    "low = re.findall(r\"\\d{2}\", str(low)) #isolates the value\n",
    "print(f\"{days[0]} High:{high[1]} Low:{low[1]}\\n{days[1]} High:{high[2]} Low:{low[2]}\\n{days[2]} High:{high[3]} Low:{low[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pumpkin Snickerdoodles\n",
      "Total time: 56 mins\n",
      "Servings: 24 servings\n",
      "Ingredients:\n",
      "2 cups (240g) all-purpose flour\n",
      "1 1/2 teaspoons cream of tartar\n",
      "1 teaspoon pumpkin pie spice\n",
      "3/4 teaspoon baking soda\n",
      "1/2 teaspoon kosher salt\n",
      "3/4 cup (170g) unsalted butter, melted and cooled\n",
      "1/2 cup (120g) pumpkin purée\n",
      "1/2 cup (107g) light brown sugar\n",
      "1/2 cup (100g) granulated sugar\n",
      "1 large egg yolk\n",
      "1 teaspoon vanilla extract\n",
      "1/4 cup (50g) granulated sugar\n",
      "1 teaspoon pumpkin pie spice\n",
      "Steps:\n",
      "1. Combine the dry ingredients:\n",
      "In a medium mixing bowl, start assembling the cookie dough. Whisk together the flour, cream of tartar, pumpkin pie spice, baking soda, and salt.\n",
      "2. Combine the wet ingredients:\n",
      "In a large mixing bowl, whisk together the melted butter, pumpkin purée, brown sugar, granulated sugar, egg yolk, and vanilla extract until fully combined and smooth.\n",
      "3. Make the batter:\n",
      "Fold the dry ingredients into the wet ingredients with a wide rubber spatula until just incorporated and no dry streaks of flour remain. At this point, the dough will seem soft.\n",
      "4. Chill:\n",
      "Cover the bowl and chill the dough in the refrigerator for at least 30 minutes. The dough can be stored in the refrigerator for up to 3 days. Chilling the dough makes it easier to scoop and shape and improves the flavor and texture of the cookies.\n",
      "5. Preheat the oven to 350°F.\n",
      "Arrange racks in the bottom and top third of the oven. Line 2 baking sheets with parchment paper.\n",
      "6. Make the pumpkin spice sugar mixture:\n",
      "In a shallow bowl, whisk together the granulated sugar and the pumpkin pie spice.\n",
      "7. Shape the cookies:\n",
      "Use a 1 1/2 tablespoon cookie scoop or a heaping tablespoon to portion the cookie dough. You should get about 24 cookies.\n",
      "Roll each scoop of cookie dough in the palms of your hand to round out and smooth it. Drop it into the sugar mixture, rolling around to coat.\n",
      "Evenly space 12 cookies on each baking sheet, about 3 inches apart.\n",
      "8. Bake:\n",
      "Place a pan on each rack in the oven and bake for 11 to 12 minutes, rotating the pans from top to bottom and back to front halfway through. The edges of the cookies will be set and the tops will look puffed and cracked.\n",
      "9. Cool and serve:\n",
      "Let the cookies cool on the pan for 5 minutes, then transfer them to a wire rack to cool completely.\n",
      "Store in an airtight container at room temperature for up to 5 days.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "text=[]\n",
    "stepdescclean=[]\n",
    "page = requests.get(\"https://www.simplyrecipes.com/pumpkin-snickerdoodles-recipe-7775402\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "title = soup.find_all(\"h1\", class_=\"heading__title\") #finds the title of the recipe using the class of the title\n",
    "text = (soup.find_all(\"span\", class_=\"meta-text__data\")) #adds all of the time and servings things to a list\n",
    "time = re.findall(r\"\\d{1,} mins\", str(text)) #puts all of the time stuff into another list\n",
    "for i in text:\n",
    "    if \"servings\" in i.text:\n",
    "        servings = i.text\n",
    "ingredients = (soup.find_all(\"li\", class_=\"structured-ingredients__list-item\")) #puts all of the ingredients stuff into a list\n",
    "print(f\"{title[0].text}\\nTotal time: {time[-1]}\\nServings: {servings}\\nIngredients:\")\n",
    "for i in ingredients:\n",
    "    print(i.text.strip())\n",
    "step = (soup.find_all(\"span\", class_=\"mntl-sc-block-subheading__text\")) #gets each step\n",
    "stepdesc = (soup.find_all(\"p\", class_=\"comp mntl-sc-block mntl-sc-block-html\")) #gets the description for each step\n",
    "for i in stepdesc: #filters out the backstory from the step description\n",
    "    if \"mntl-sc-block_3\" in i[\"id\"]:\n",
    "        stepdescclean.append(i)\n",
    "stepnum = 1\n",
    "idnumber=0\n",
    "print(\"Steps:\")\n",
    "for i in range(len(step)):\n",
    "    print(f\"{stepnum}. {step[i].text.strip()}\")\n",
    "    print(stepdescclean[i].text.strip()) #prints the step description\n",
    "    idnumber = int(stepdescclean[i][\"id\"][-2:].strip(\"-\")) #saves the last one or two numbers of the step description as idnumber\n",
    "    try:\n",
    "        while int(stepdescclean[i+1][\"id\"][-2:].strip(\"-\")) == int(idnumber)+1: #if the idnumber of the next step description is one more than the current idnumber...\n",
    "            if \"leave us stars\" in stepdescclean[i+1].text.strip().lower():\n",
    "                break\n",
    "            print(stepdescclean[i+1].text.strip()) #it will print the next stepdescription before printing the next step\n",
    "            idnumber+=1\n",
    "            stepdescclean.pop(i+1)\n",
    "    except IndexError:\n",
    "        break\n",
    "    stepnum+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
